---
title: Novel Factor Target Gene Prediction
subtitle: through Multi-Omics Datasets
author: "A. Kapoor, D. Langreiter, S. Udit, L. Richard"
date: "University of Sydney | OLET5602 | `r format(Sys.time(), '%B, %Y')`"
output:
  html_document:
    toc: true
    toc_float: true
    code_folding: hide
    theme:
      bg: "white"
      fg: "black"
      primary: "#050A30"
      base_font:
        google: Montserrat
      heading_font:
        google: Merriweather
  pdf_document: default
    extra_dependencies: ["geometry"]
header-includes:
  - \usepackage{listings}
  - \lstset{breaklines=true}
---

# Introduction

**Aim:**
The aim of the project is to classify genes as targets for novel transcription factors. The focus of the study in particularly on Sox2 and Nanog (of the OSN Factors::w Oct4, Sox2, Nanog) embryonic stem cell (ESC) differentiation. 

This is a particularly important, and ongoing, area of research as there is some evidence that OSN factors are regulators of stem cell maintenance.

**Background:**

**Background:**

Embryonic stem cells (ESCs) differentiate into a diverse array of cell types, a process fundamental to development. The regulation of ESC differentiation and cell fate decisions is intricately controlled by transcription factors. These transcription factors operate within complex transcriptional networks that influence gene expression through direct binding to DNA and interaction with other regulatory proteins ([Theunissen and Jaenisch, 2017](https://doi.org/10.1038/nrm.2017.15)).

Transcriptional regulation is crucial in defining cell identity and function during differentiation. Transcription factors can act as activators/enhancers or repressors/silencers, modulating the expression of genes that drive cell lineage specification. Some studies have shown that Sox2 ([Masui et al., 2007](https://doi.org/10.1038/nature06015)) and Nanog ([Masui et al., 2007](https://doi.org/10.1038/nature06015)) are key regulators of stem cell maintenance and differentiation. Oct4, Sox2, and Nanog, collectively known as the 'OSN' factors, are well-established in their roles as regulators of stem cell maintenance ([Yeo and Ng, 2012](https://doi.org/10.1038/nrm3430)).

Recent advancements in omics technologies have enabled high-temporal-resolution profiling of genome-wide transcriptional and epigenetic events. The time-course multi-omic profiling of ESC differentiation provides a unique opportunity to reveal previously unknown aspects of stem cells during pluripotency progression ([Yang et al., 2019](https://doi.org/10.1016/j.cell.2019.02.035)). Multi-omics approaches, integrating genomics, transcriptomics, proteomics, and epigenomics, can provide us unprecedented insights into the regulatory networks governing pluripotency and differentiation.

In this study, we aim to predict novel target genes of Sox2 and Nanog by leveraging multi-omics data from ESC differentiation experiments. By analyzing changes in gene expression, protein interactions, and epigenetic modifications, we seek to identify new substrates of these critical transcription factors. This approach will enhance our understanding of the transcriptional regulation of ESC differentiation and contribute to the development of more precise strategies for manipulating stem cell behavior in regenerative medicine.

**Dataset Overview:**

The data collects time-course differentiation of genes at various omics layers. The ones that the study will focus on are below.

For the Purposes of this study we refer to OSN Labels as genes that target for transcription factors: Sox2 and Nanog.

- **Transcriptome:** Time-course mRNA profiles during ESC differentiation.

- **Proteome:** Time-course protein expression profiles during ESC differentiation.

- **Epigenome:** Time-course ESC differentation epigonme profiles of 6 histone marks.

# EDA
## Load Required Libraries and Data

We start by loading the necessary R packages and the dataset `Final_Project_ESC.RData`, which contains the transcriptome, proteome, and epigenome data, along with a subset of known Sox2/Nanog target genes.

```{r setup, echo=FALSE}

knitr::opts_chunk$set(echo = TRUE)

htmltools::tags$style("
  pre {
    white-space: pre-wrap;      /* Wrap text within pre blocks */
    word-wrap: break-word;      /* Allows long words to be broken and wrapped */
    overflow-wrap: break-word;  /* Ensures wrapping occurs at the edge of the container */
  }
")
```

```{r}
# Load necessary packages and data
load("Final_Project_ESC.RData", verbose = TRUE)

suppressPackageStartupMessages({
    library(e1071)
    library(ggplot2)
    library(ROCR)
    library(calibrate)
    library(dplyr)
    library(tibble)
    library(reshape2)
    library(kernlab)
    library(caret)
    library(randomForest)
    library(adabag)
    library(gbm)
    library(xgboost)
    library(nnet)
    library(pROC)
    library(doParallel)
    library(calibrate)
})

set.seed(123)

```

## Describe and explore the Data set details

Before beginning data analysis it is important understand and investigate the data. The goal of this report is to predict to predict novel transcription factor target genes from multi-omics data. For each of our datasets, one can look at the structure of the data and perform PCA Analysis as means of identifying trends in the dataset.

Below is the temporal expression of the Triptome, Proteome, and Epigiome datasets at timepoints 0, 1, 6, 12, 24, 36, 48 hours.



### Transcriptome

```{r}
head(Transcriptome)
dim(Transcriptome)
colnames(Transcriptome)

# PCA analysis on the correlation matrix of the transcriptome data
cor.mat <- cor(Transcriptome)
pca.mat <- prcomp(cor.mat)

# Plot the PCA
grp <- rownames(pca.mat$x)
grp.col <- rainbow(nrow(pca.mat$x))
names(grp.col) <- rownames(pca.mat$x)

# Generate PCA plot
plot(pca.mat$x[,1], pca.mat$x[,2], col=grp.col[grp], pch=19, cex=2,
     xlab=paste0("PC1 (", round(summary(pca.mat)$importance[2,1]*100,1), "% variance)"),
     ylab=paste0("PC2 (", round(summary(pca.mat)$importance[2,2]*100,1), "% variance)"))

# Add sample labels to the plot
calibrate::textxy(pca.mat$x[,1], pca.mat$x[,2], labs=grp, cex=0.5)

### Transcriptome Exploration 

The transcriptome dataset documents the mRNA expression of 19,788 genes during embryonic stem cell differentiation at 8 time points (0 h to 72 h). At each time point, changes in gene activity were reflected as the cells progressed from a pluripotent state to a more differentiated stage.
The PCA plots demonstrate the variance of gene expression at different time points. PC1 explains 61.4% of the variance, while PC2 accounts for 29.9%, meaning that these two principal components together capture most of the dynamic changes in the transcriptome data.
The clear trajectory from 0 to 72 hours shows a clear time course of gene expression, which reflects the onset of the differentiation process.


```

### Proteome

```{r}
cor.proteome <- cor(Proteome)
pca.proteome <- prcomp(cor.proteome)
summary(pca.proteome)$importance

# Using the previous correlation matrix and PCA results
cor.proteome <- cor(Proteome)
pca.proteome <- prcomp(cor.proteome)

# Get group labels and colors
grp <- rownames(pca.proteome$x)  # Set groups according to your data
grp.col <- rainbow(nrow(pca.proteome$x))
names(grp.col) <- rownames(pca.proteome$x)

# Plot the PCA
plot(pca.proteome$x[,1], pca.proteome$x[,2], col=grp.col[grp], pch=19, cex=2,
     xlab=paste0("PC1 (", round(summary(pca.proteome)$importance[2,1]*100,1), "% variance)"),
     ylab=paste0("PC2 (", round(summary(pca.proteome)$importance[2,2]*100,1), "% variance)"))

```
### Proteome Exploration 
In the PCA analysis of proteomics, PC1 explained 67.6% of the variance, capturing most of the variation in the proteomic data.
PC2  explained 21% of the variance, further revealing another layer of variation in protein expression differences. 

The PCA plot shows a separation of samples on the PC1 and PC2 axes, indicating significant changes in protein expression over time. The trajectories in the PCA plot show gradual changes in protein levels as cells differentiate. 


### h3k4me3

```{r}
# PCA analysis on the correlation matrix of the H3K4me3 data
cor.h3k4me3 <- cor(H3K4me3)
pca.h3k4me3 <- prcomp(cor.h3k4me3)

# Get group labels and colors
grp <- rownames(pca.h3k4me3$x)
grp.col <- rainbow(nrow(pca.h3k4me3$x))
names(grp.col) <- rownames(pca.h3k4me3$x)

# Generate PCA plot for H3K4me3
plot(pca.h3k4me3$x[,1], pca.h3k4me3$x[,2], col=grp.col[grp], pch=19, cex=2,
     xlab=paste0("PC1 (", round(summary(pca.h3k4me3)$importance[2,1]*100,1), "% variance)"),
     ylab=paste0("PC2 (", round(summary(pca.h3k4me3)$importance[2,2]*100,1), "% variance)"))

# Add sample labels to the plot
calibrate::textxy(pca.h3k4me3$x[,1], pca.h3k4me3$x[,2], labs=grp, cex=0.5)

### h3k4me3 Exploration
By analyzing changes at different time points, the H3K4me3 dataset helps us understand chromatin dynamics during embryonic stem cell differentiation. The dataset covers histone modification levels at different time points (e.g., 1 h, 6 h, 24 h, 36 h, 48 h).

PCA showed that PC1 explained 77.9% of the variance and PC2 explained 18.1%, suggesting that these two components capture most of the dynamic changes in H3K4me3 modifications during differentiation.

PCA plots show the distribution of H3K4me3 modification patterns at each time point. Similar time points (e.g., H3K4me3_36h and H3K4me3_48h) are clustered together, indicating that they have similar modification patterns. In contrast, time points such as H3K4me3_1h and H3K4me3_24h are more dispersed, suggesting that histone modification patterns differ significantly at these stages of differentiation.

```

### H3K27me3

```{r}

# PCA analysis for H3K27me3 data
cor.h3k27me3 <- cor(H3K27me3)
pca.h3k27me3 <- prcomp(cor.h3k27me3)

# Update group labels and colors for H3K27me3
grp.h3k27me3 <- rownames(pca.h3k27me3$x)
grp.col.h3k27me3 <- rainbow(nrow(pca.h3k27me3$x))
names(grp.col.h3k27me3) <- rownames(pca.h3k27me3$x)

# Generate PCA plot for H3K27me3
plot(pca.h3k27me3$x[,1], pca.h3k27me3$x[,2], col=grp.col.h3k27me3[grp.h3k27me3], pch=19, cex=2,
     xlab=paste0("PC1 (", round(summary(pca.h3k27me3)$importance[2,1]*100,1), "% variance)"),
     ylab=paste0("PC2 (", round(summary(pca.h3k27me3)$importance[2,2]*100,1), "% variance)"))

# Correctly label the samples for H3K27me3
calibrate::textxy(pca.h3k27me3$x[,1], pca.h3k27me3$x[,2], labs=grp.h3k27me3, cex=0.5)

### H3K27me3 Exploration
The H3K27me3 dataset, capturing histone modification levels at pivotal time points from 0 to 48 hours, is instrumental in studying the silencing of chromatin during cellular differentiation. PCA shows the variance with PC1 accounting for 91.2% and PC2 for 8.1%, thereby demonstrating the major changes in H3K27me3 markers. 

The clustering observes in later stages like 36 and 48 hours in the PCA plot reflects a convergence in gene repression patterns, while the divergence at the early 0 and 1-hour marks indicates a distinct onset of repression. 

```

### H3K27ac

```{r}

# PCA analysis for H3K27ac data
cor.h3k27ac <- cor(H3K27ac)
pca.h3k27ac <- prcomp(cor.h3k27ac)

# Update group labels and colors for H3K27ac
grp.h3k27ac <- rownames(pca.h3k27ac$x)
grp.col.h3k27ac <- rainbow(nrow(pca.h3k27ac$x))
names(grp.col.h3k27ac) <- rownames(pca.h3k27ac$x)

# Generate PCA plot for H3K27ac
plot(pca.h3k27ac$x[,1], pca.h3k27ac$x[,2], col=grp.col.h3k27ac[grp.h3k27ac], pch=19, cex=2,
     xlab=paste0("PC1 (", round(summary(pca.h3k27ac)$importance[2,1]*100,1), "% variance)"),
     ylab=paste0("PC2 (", round(summary(pca.h3k27ac)$importance[2,2]*100,1), "% variance)"))

# Correctly label the samples for H3K27ac
calibrate::textxy(pca.h3k27ac$x[,1], pca.h3k27ac$x[,2], labs=grp.h3k27ac, cex=0.5)

### H3K27ac Exploration
H3K27ac is a marker active enhancer histone modification that is important for modulating gene expression during cellular differentiation. This dataset tracks histone modification changes at key time points from 1 hour to 48 hours.By PCA, PC1 explained 84% of the variance while PC2 explained 13.5%, which together captured the major trends in H3K27ac modification patterns over time.

The PCA plots shows that later time points such as 36 hours and 48 hours clustered together, indicating similar enhancer activity at these time points. In contrast, early time points such as 1 hr and 6 hr were significantly different, reflecting unique histone modification patterns at the beginning of differentiation. 
```


### H3K4me1

```{r}
# PCA analysis for H3K4me1 data
cor.h3k4me1 <- cor(H3K4me1)
pca.h3k4me1 <- prcomp(cor.h3k4me1)

# Define the group labels and colors specifically for H3K4me1 data
grp.h3k4me1 <- rownames(pca.h3k4me1$x)
grp.col.h3k4me1 <- rainbow(nrow(pca.h3k4me1$x))
names(grp.col.h3k4me1) <- rownames(pca.h3k4me1$x)

# Generate PCA plot for H3K4me1
plot(pca.h3k4me1$x[,1], pca.h3k4me1$x[,2], col=grp.col.h3k4me1[grp.h3k4me1], pch=19, cex=2,
     xlab=paste0("PC1 (", round(summary(pca.h3k4me1)$importance[2,1]*100,1), "% variance)"),
     ylab=paste0("PC2 (", round(summary(pca.h3k4me1)$importance[2,2]*100,1), "% variance)"))

# Correctly label the samples for H3K4me1
calibrate::textxy(pca.h3k4me1$x[,1], pca.h3k4me1$x[,2], labs=grp.h3k4me1, cex=0.5)

### H3K4me1 Exploration
H3K4me1 is a type of histone modification found in epigenomic data, playing a role in regulating gene expression during cellular differentiation. The dataset was collected at key time points, including 0, 1, 6, 12, 24, 36, and 48 hours, allowing for the examination of changes in chromatin states. 

Through PCA, PC1 shows 85% of the variance, while PC2 shows 11.7%. The PCA plot indicates that similar enhancer activity patterns are observed in later stages such as 36 and 48 hours, whereas distinct enhancer modification patterns are exhibited in the early stages like 0 and 1 hour. 


```

### H3K9me2

```{r}
# PCA analysis for H3K9me2 data
cor.h3k9me2 <- cor(H3K9me2)
pca.h3k9me2 <- prcomp(cor.h3k9me2)

# Define the group labels and colors specifically for H3K9me2 data
grp.h3k9me2 <- rownames(pca.h3k9me2$x)
grp.col.h3k9me2 <- rainbow(nrow(pca.h3k9me2$x))
names(grp.col.h3k9me2) <- rownames(pca.h3k9me2$x)

# Generate PCA plot for H3K9me2
plot(pca.h3k9me2$x[,1], pca.h3k9me2$x[,2], col=grp.col.h3k9me2[grp.h3k9me2], pch=19, cex=2,
     xlab=paste0("PC1 (", round(summary(pca.h3k9me2)$importance[2,1]*100,1), "% variance)"),
     ylab=paste0("PC2 (", round(summary(pca.h3k9me2)$importance[2,2]*100,1), "% variance)"))

# Correctly label the samples for H3K9me2
calibrate::textxy(pca.h3k9me2$x[,1], pca.h3k9me2$x[,2], labs=grp.h3k9me2, cex=0.5)

### H3K9me2 Exploration
H3K9me2 is a histone modification associated with gene repression that plays a role in maintaining heterochromatin structure during cell differentiation. Data were collected at multiple key time points including 0 hr, 1 hr, 6 hr, 12 hr, 24 hr, 36 hr, and 48 hr to monitor changes in this epigenetic mark.

PCA showed that PC1 explained 83.6% of the variance, and PC2 explained an additional 13.1%. PCA plots revealed clustering at later stages, with data from 36 hr and 48 hr clustered together, suggesting similar levels of gene suppression activity. In contrast, early stages like 0 h and 1 h showed different patterns of repression.

```


### PolII

```{r}
# PCA analysis for PolII data
cor.polii <- cor(PolII)
pca.polii <- prcomp(cor.polii)

# Define the group labels and colors specifically for PolII data
grp.polii <- rownames(pca.polii$x)
grp.col.polii <- rainbow(nrow(pca.polii$x))
names(grp.col.polii) <- rownames(pca.polii$x)

# Generate PCA plot for PolII
plot(pca.polii$x[,1], pca.polii$x[,2], col=grp.col.polii[grp.polii], pch=19, cex=2,
     xlab=paste0("PC1 (", round(summary(pca.polii)$importance[2,1]*100,1), "% variance)"),
     ylab=paste0("PC2 (", round(summary(pca.polii)$importance[2,2]*100,1), "% variance)"))

# Correctly label the samples for PolII
calibrate::textxy(pca.polii$x[,1], pca.polii$x[,2], labs=grp.polii, cex=0.5)

### PolII Exploration

PolII is responsible for transcribing DNA into mRNA, and its activity plays a crucial role in regulating gene expression during cellular differentiation. Data collected at various time points such as 0, 1, 6, 12, 24, 36, and 72 hours reveal the dynamic changes in PolII activity. PCA indicates that PC1 accounts for 60.7% of the variance, while PC2 accounts for 26%. 

The PCA results show that in later stages like 36 and 72 hours, PolII binding patterns are similar, as indicated by their clustering together; whereas in the early stages such as 0 and 1 hour, PolII activity patterns are distinctly different, suggesting variations in the transcription initiation process.

```

# Classification

## Filter and Combine Datasets
In order to properly model and predicting novel transcription factors target genes we join the 8 datasets together and perform our calculations on the larger dataset.

To ensure consistency across datasets, we filter each dataset to include only the common genes present in all omics layers. We then combine these filtered datasets into a single data frame for further analysis.

```{r}
# Ensure all data has the same set of genes
genes <- intersect(rownames(Transcriptome), rownames(Proteome))
genes <- intersect(genes, rownames(H3K4me3))
genes <- intersect(genes, rownames(H3K27me3))
genes <- intersect(genes, rownames(H3K27ac))
genes <- intersect(genes, rownames(H3K4me1))
genes <- intersect(genes, rownames(H3K9me2))
genes <- intersect(genes, rownames(PolII))
```


```{r}
# Filter each dataset for the common genes
Transcriptome_filter <- Transcriptome[genes, ]
Proteome_filter <- Proteome[genes, ]
H3K4me3_filter <- H3K4me3[genes, ]
H3K27me3_filter <- H3K27me3[genes, ]
H3K27ac_filter <- H3K27ac[genes, ]
H3K4me1_filter <- H3K4me1[genes, ]
H3K9me2_filter <- H3K9me2[genes, ]
PolII_filter <- PolII[genes, ]

# Confirm that all datasets share the same gene
identical(rownames(Transcriptome_filter), rownames(H3K4me3_filter))
identical(rownames(Proteome_filter), rownames(H3K4me3_filter))
identical(rownames(H3K27ac_filter), rownames(H3K4me3_filter))
identical(rownames(H3K4me1_filter), rownames(H3K4me3_filter))
identical(rownames(H3K9me2_filter), rownames(H3K4me3_filter))
identical(rownames(PolII_filter), rownames(H3K4me3_filter))
```

```{r}
# Rename columns to avoid conflicts
colnames(Transcriptome_filter) <- paste("T_", colnames(Transcriptome_filter), sep = "")
colnames(Proteome_filter) <- paste("P_", colnames(Proteome_filter), sep = "")
colnames(H3K4me3_filter) <- paste("H3K4me3_", colnames(H3K4me3_filter), sep = "")
colnames(H3K27me3_filter) <- paste("H3K27me3_", colnames(H3K27me3_filter), sep = "")
colnames(H3K27ac_filter) <- paste("H3K27ac_", colnames(H3K27ac_filter), sep = "")
colnames(H3K4me1_filter) <- paste("H3K4me1_", colnames(H3K4me1_filter), sep = "")
colnames(H3K9me2_filter) <- paste("H3K9me2_", colnames(H3K9me2_filter), sep = "")
colnames(PolII_filter) <- paste("PolII_", colnames(PolII_filter), sep = "")

# Combine the datasets
combined_data <- cbind(
  Transcriptome_filter,
  Proteome_filter,
  H3K4me3_filter,
  H3K27me3_filter,
  H3K27ac_filter,
  H3K4me1_filter,
  H3K9me2_filter,
  PolII_filter
)

# Add the labels
label <- ifelse(genes %in% OSN_target_genes_subset, "OSN", "Other")
combined_data <- data.frame(combined_data)
combined_data$label <- factor(label)
```


```{r}
# Number of genes which are known to be targets for Sox2 and Nanog
length(OSN_target_genes_subset)
```


We have 100 known target genes for OSN, and as seen below the we have 95 genes that have been identified as novel Sox2/Nanog targets on our combined filtered dataset.

```{r}
# Check the initial label distribution
print(table(combined_data$label))
```

### Data Splitting and Balancing

The dataset is split into training (90%) and testing (10%) sets. The label column is reassigned to the test set to ensure that it is included correctly.

```{r}
# Split the dataset into training (90%) and testing (10%) sets
set.seed(123)
train_index <- createDataPartition(combined_data$label, p = 0.9, list = FALSE)

train_data <- combined_data[train_index, ]
test_data <- combined_data[-train_index, ]

# Reassign the label column to test_data
test_data$label <- combined_data[-train_index, "label"]

# Check the distribution of labels in the training and test sets
print("Training set label distribution:")
print(table(train_data$label))

print("Test set label distribution:")
print(table(test_data$label))

```

### Balancing the Training Data

To address the imbalance in the dataset, downsampling is employed to ensure both classes, `OSN` and `Other`, are represented equally. This technique enhances model accuracy and generalization by preventing bias towards the more frequent class.

```{r}
# Balance the training data using downsampling
set.seed(123)
downsampled_train_data <- downSample(x = train_data[, -ncol(train_data)],
                                     y = train_data$label,
                                     list = FALSE, yname = "label")

# Display the new balanced label distribution
print("Balanced training set label distribution:")
table(downsampled_train_data$label)

# Display dimensions of the balanced training data
dim(downsampled_train_data)
```

```{r}
# Final check of training dataset dimensions
print(dim(downsampled_train_data))

```

## Model Training
<<<<<<< HEAD
=======
We train two machine learning models, SVM (with the radial kernel) and Random Forest, using the balanced training dataset.
>>>>>>> 93663ae (Add theme and format)

Two models, SVM with a radial kernel and Random Forest, are trained using the balanced dataset to compare their performance under balanced class distribution conditions.

```{r}
# Train an SVM model on the downsampled training data with radial basis function kernel
svm_model <- svm(label ~ ., data = downsampled_train_data, kernel = "radial", probability = TRUE)

# Train a Random Forest model
rf_model <- randomForest(label ~ ., data = downsampled_train_data, ntree = 1000)

# Extract and plot feature importance
importance <- importance(rf_model)
ord <- order(importance, decreasing = TRUE)
barplot(importance[ord], main = "Feature Importance in Random Forest", col = 'blue')

```

We can then perform corss-validation and finetune the hyperparameters, being careful not to overfit the model.

```{r}

# Define tuning grid and control setup
tuning_grid <- expand.grid(mtry = seq(2, 5, by = 1))
control <- trainControl(method = "cv", number = 5)

# Tuning the model
set.seed(123)
tuned_rf_model <- train(label ~ ., data = downsampled_train_data,
                        method = "rf", trControl = control, tuneGrid = tuning_grid)

# Plotting tuning results
plot(tuned_rf_model$finalModel, main="Random Forest Tuning")

```

Before conducting proper evaluation later on this report, we can briefly evaluate the performance of this model.

```{r}

rf_predictions <- predict(tuned_rf_model, newdata = test_data, type = "prob")
roc_curve <- roc(response = test_data$label, predictor = as.numeric(rf_predictions[,2]))

# Plot ROC curve
plot(roc_curve, main = "ROC Curve for Random Forest")

```

Upon viewing the performance of the model above, it was clear that more could be done to optimise the model. As a result, the team decided to experiment with other models to evaluate the effectiveness of variations of standard random forest models.

### Bagging w/ Bagged Trees

Bagging improves stability and accuracy by reducing variance and avoiding overfitting.

```{r}

bagged_trees <- train(
  label ~ .,
  data = downsampled_train_data,
  method = "treebag",
  trControl = trainControl(method = "cv", number = 5),
  tuneLength = 5
)
print(bagged_trees)

```

### Gradient Boosting with Hyperparameter Tuning Using xgboost (using parallel processing)

For hyperparameters tuning we define the grid of paramaters below and train an XGB Model on the downsampled data. We use cross validation as means to providing a reliable model that is not overfit, and allow the model to be trained in parallel.

```{r}

# Set up a tuning grid for xgboost
tune_grid_xgb <- expand.grid(
  nrounds = c(100, 200),
  max_depth = c(3, 6),
  eta = c(0.1, 0.3),
  gamma = c(0, 0.1),
  colsample_bytree = c(0.5, 1),
  min_child_weight = c(1, 10),
  subsample = c(0.5, 1)
)

# Enable parallel processing
library(doParallel)
cl <- makeCluster(detectCores())
registerDoParallel(cl)

# Define control function for training
train_control <- trainControl(
  method = "cv",
  number = 3,
  savePredictions = "final",
  verboseIter = TRUE,
  allowParallel = TRUE
)

# Train xgboost model with tuning
library(caret)
xgb_model_tuned <- train(
  label ~ .,
  data = downsampled_train_data,
  method = "xgbTree",
  trControl = train_control,
  tuneGrid = tune_grid_xgb,
  metric = "Accuracy"
)
print(xgb_model_tuned$bestTune)

# Plot model performance
plot(xgb_model_tuned)

```

## Extended Model Training - Neural Networks

To extend our understanding of the dataset and compare our Random Forest models against other classifiers, the team decided to evaluate the use of a neural network on the same training data.

### Preparing Data and Feature Scaling

Proper data scaling is necessary for the performance of a Neural Network due to the sensitivity of the implementation of the algorithm in R to the scale of input variables.

```{r}

scaled_data <- scale(downsampled_train_data[, -ncol(downsampled_train_data)])
scaled_train_data <- data.frame(scaled_data, label = downsampled_train_data$label)

```

Then, as was done with the Random Forest model, the team trained the Neural Network with varying architectural parameters and visualizing the tuning process to identify the best model settings.

```{r}

# Setup for Neural Network training
control_nn <- trainControl(method = "cv", number = 5, savePredictions = "final")
grid_nn <- expand.grid(.size = c(5, 10), .decay = c(0.1, 0.01))

# Train the Neural Network
set.seed(123)
nn_model <- train(label ~ ., data = scaled_train_data, method = "nnet", trControl = control_nn, tuneGrid = grid_nn, trace = FALSE)

```

We can then plot the model's performance.

```{r}

plot(nn_model)

```

## Model Evaluation
Predictions and Confusion Matrix
We use the trained models to make predictions on the test set and evaluate their performance using confusion matrices.

```{r}
# Ensure column names in the test set match the training data
colnames(test_data) <- colnames(downsampled_train_data)[1:(ncol(downsampled_train_data) - 1)]  # Exclude the label column

# Check if the label column is present and correctly populated
if ("label" %in% colnames(combined_data) && length(combined_data[-train_index, "label"]) == nrow(test_data)) {
    # Assign the label to test_data
    test_data$label <- combined_data[-train_index, "label"]
} else {
    stop("The label vector is empty or has a different length than expected. Check the data preparation steps.")
}

# Ensure the label is a factor with the correct levels
test_data$label <- factor(test_data$label, levels = c("OSN", "Other"))

# SVM Predictions on the test set
svm_test_pred <- predict(svm_model, newdata = test_data[, -ncol(test_data)], probability = TRUE)
svm_test_prob <- attr(svm_test_pred, "probabilities")[, "OSN"]

# Random Forest Predictions on the test set
rf_test_pred <- predict(rf_model, newdata = test_data[, -ncol(test_data)], type = "prob")[, "OSN"]

# SVM Confusion Matrix on the test set
svm_test_conf_matrix <- table(Predicted = ifelse(svm_test_prob > 0.5, "OSN", "Other"), Actual = test_data$label)
print("SVM Test Confusion Matrix:")
print(svm_test_conf_matrix)

# Random Forest Confusion Matrix on the test set
rf_test_conf_matrix <- table(Predicted = ifelse(rf_test_pred > 0.5, "OSN", "Other"), Actual = test_data$label)
print("Random Forest Test Confusion Matrix:")
print(rf_test_conf_matrix)

```

## ROC Curve and AUC
To further assess model performance, we plot the ROC curves and calculate the AUC for both the SVM and Random Forest models.

```{r}
# Evaluate the models on the test set (AUC and ROC)
svm_test_roc <- roc(test_data$label, svm_test_prob)
plot(svm_test_roc, main = "SVM ROC Curve")
print(paste("Final SVM Test AUC:", auc(svm_test_roc)))

rf_test_roc <- roc(test_data$label, rf_test_pred)
plot(rf_test_roc, main = "Random Forest ROC Curve")
print(paste("Final Random Forest Test AUC:", auc(rf_test_roc)))

```


## Future Directions:

**Model Improvement:**
Further hyperparameter tuning using automated methods like grid search or random search could refine the model's accuracy. Additionally, exploring ensemble methods that combine predictions from several models might yield better results.

**Data Expansion:**
Incorporating additional omics layers, such as metabolomics or additional transcription factor binding profiles, could help to improve the model's predictive power and generalisability.

**Integration with Clinical Data:**
Linking omics profiles with clinical outcomes could also be explored to improve the translational impact of the research.
